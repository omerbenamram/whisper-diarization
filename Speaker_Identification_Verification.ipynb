{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "NEMO_ROOT = os.getcwd()\n",
    "print(NEMO_ROOT)\n",
    "import glob\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "data_dir = os.path.join(NEMO_ROOT,'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5FPmxUkGakD"
   },
   "source": [
    "## Path to manifest files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vo-VnYPtJO_v"
   },
   "outputs": [],
   "source": [
    "train_manifest = 'train_manifest.json'\n",
    "validation_manifest = 'dev_manifest.json'\n",
    "test_manifest = 'test_manifest.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJtU_GEdMUUo"
   },
   "source": [
    "# Training\n",
    "Import necessary packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All the following steps are just for explanation of each section, but one can use the provided [training script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco.py) to launch training in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1ojB0cZMSmv"
   },
   "outputs": [],
   "source": [
    "import nemo\n",
    "# NeMo's ASR collection - This collection contains complete ASR models and\n",
    "# building blocks (modules) for ASR\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5Zho11LNAFJ"
   },
   "source": [
    "## Model Configuration \n",
    "The TitaNet model is defined in a config file which declares multiple important sections.\n",
    "\n",
    "They are:\n",
    "\n",
    "1) model: All arguments that will relate to the Model - preprocessors, encoder, decoder, optimizer and schedulers, datasets, and any other related information\n",
    "\n",
    "2) trainer: Any argument to be passed to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will print the entire config of sample TitaNet model\n",
    "!mkdir conf \n",
    "!wget -P conf https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/recognition/conf/titanet-large.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/titanet-large.yaml')\n",
    "config = OmegaConf.load(MODEL_CONFIG)\n",
    "config.model.train_ds.manifest_filepath = train_manifest\n",
    "config.model.validation_ds.manifest_filepath = validation_manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/SpeakerNet_recognition_3x2x512.yaml')\n",
    "config = OmegaConf.load(MODEL_CONFIG)\n",
    "config.model.train_ds.manifest_filepath = train_manifest\n",
    "config.model.validation_ds.manifest_filepath = validation_manifest\n",
    "config.model.test_ds.manifest_filepath = test_manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HQtZfKnMhpI"
   },
   "outputs": [],
   "source": [
    "print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since we are training speaker embedding extractor model for verification we do not add test_ds dataset. To include it add it to config and replace manifest file as \n",
    "`config.model.test_ds.manifest_filepath = test_manifest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-B96tFTnW8Yh"
   },
   "outputs": [],
   "source": [
    "config.model.decoder.num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83pHBRDpQTF0"
   },
   "source": [
    "## Building the PyTorch Lightning Trainer\n",
    "NeMo models are primarily PyTorch Lightning modules - and therefore are entirely compatible with the PyTorch Lightning ecosystem!\n",
    "\n",
    "Let us first instantiate a Trainer object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WIYf4-KFQYHl"
   },
   "outputs": [],
   "source": [
    "print(\"Trainer config - \\n\")\n",
    "print(OmegaConf.to_yaml(config.trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXuSMYMNQeW7"
   },
   "outputs": [],
   "source": [
    "# Let us modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "config.trainer.devices = 1\n",
    "config.trainer.accelerator = accelerator\n",
    "\n",
    "# Reduces maximum number of epochs to 5 for quick demonstration\n",
    "config.trainer.max_epochs = 50\n",
    "\n",
    "# Remove distributed training flags\n",
    "config.trainer.strategy = None\n",
    "\n",
    "# Remove augmentations\n",
    "config.model.train_ds.augmentor=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBq3eCLwQhCy"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(**config.trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xHq_rcmQiry"
   },
   "source": [
    "## Setting up a NeMo Experiment\n",
    "NeMo has an experiment manager that handles logging and checkpointing for us, so let's use it !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMm8MPYfQsCS"
   },
   "outputs": [],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "log_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
    "# The log_dir provides a path to the current logging directory for easy access\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_KY_s5LROYf"
   },
   "outputs": [],
   "source": [
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=config.model, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HvYhsOWuSpL_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(speaker_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvtVKO8FZsoe"
   },
   "source": [
    "If you have a test manifest file, we can easily compute test accuracy by running\n",
    "<pre><code>trainer.test(speaker_model, ckpt_path=None)\n",
    "</code></pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(speaker_model, ckpt_path=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcnWub9-0TW2"
   },
   "source": [
    "## Saving/Restoring a checkpoint\n",
    "There are multiple ways to save and load models in NeMo. Since all NeMo models are inherently Lightning Modules, we can use the standard way that PyTorch Lightning saves and restores models.\n",
    "\n",
    "NeMo also provides a more advanced model save/restore format, which encapsulates all the parts of the model that are required to restore that model for immediate use.\n",
    "\n",
    "In this example, we will explore both ways of saving and restoring models, but we will focus on the PyTorch Lightning method.\n",
    "\n",
    "## Saving and Restoring via PyTorch Lightning Checkpoints\n",
    "When using NeMo for training, it is advisable to utilize the exp_manager framework. It is tasked with handling checkpointing and logging (Tensorboard as well as WandB optionally!), as well as dealing with multi-node and multi-GPU logging.\n",
    "\n",
    "Since we utilized the exp_manager framework above, we have access to the directory where the checkpoints exist.\n",
    "\n",
    "exp_manager with the default settings will save multiple checkpoints for us -\n",
    "\n",
    "1) A few checkpoints from certain steps of training. They will have --val_loss= tags\n",
    "\n",
    "2) Checkpoints at the last epoch of training are denoted by --last.\n",
    "\n",
    "3) If the model finishes training, it will also have a --last checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSLjq-edaPt_"
   },
   "outputs": [],
   "source": [
    "# Let us list all the checkpoints we have\n",
    "checkpoint_dir = os.path.join(log_dir, 'checkpoints')\n",
    "checkpoint_paths = list(glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\")))\n",
    "checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BwltdVWXaroa"
   },
   "outputs": [],
   "source": [
    "final_checkpoint = list(filter(lambda x: \"-last.ckpt\" in x, checkpoint_paths))[0]\n",
    "print(final_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tGKKojs0fEh"
   },
   "source": [
    "\n",
    "## Restoring from a PyTorch Lightning checkpoint\n",
    "To restore a model using the LightningModule.load_from_checkpoint() class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyP9cYVbFc8"
   },
   "outputs": [],
   "source": [
    "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.load_from_checkpoint(final_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AnZVMKZpbI_M"
   },
   "source": [
    "# Finetuning\n",
    "Since we don't have any new manifest file to finetune, I will demonstrate here by using the test manifest file we created earlier. \n",
    "an4 test dataset has a different set of speakers from the train set (total number: 10). And as we didn't split this dataset for validation I will use the same for validation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV9gInFwQ2F5"
   },
   "source": [
    "There are a couple of ways we can finetune a speaker recognition model. \n",
    "1. Finetuning using a pretrained model published on NGC. \n",
    "2. Finetuning from a PTL checkpoint. \n",
    "\n",
    "Since finetuning from a large pretrained model is more common, I shall use it to demonstrate finetuning procedure. In order to make finetuning step independent from training from scratch, we use another config. Here we shall use `titanet-finetune.yaml` config, that is created to show finetuning on pretrained titanet-large model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You may use [finetune-script](https://github.com/NVIDIA/NeMo/blob/main/examples/speaker_tasks/recognition/speaker_reco_finetune.py) to launch training in the command line. Following is just a demonstration of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -P conf https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/recognition/conf/titanet-finetune.yaml\n",
    "MODEL_CONFIG = os.path.join(NEMO_ROOT,'conf/titanet-finetune.yaml')\n",
    "finetune_config = OmegaConf.load(MODEL_CONFIG)\n",
    "print(OmegaConf.to_yaml(finetune_config))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For step 2, if one would like to finetune from a PTL checkpoint, `init_from_pretrained_model` in config should be replaced with `init_from_nemo_model` and need to provide the path to checkpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_config.model.train_ds.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtXUWmYLQ0PJ"
   },
   "outputs": [],
   "source": [
    "finetune_config.model.train_ds.manifest_filepath = test_manifest\n",
    "finetune_config.model.validation_ds.manifest_filepath = test_manifest\n",
    "finetune_config.model.decoder.num_classes = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHy1zE1cTDZn"
   },
   "source": [
    "So we have set up the data and changed the decoder required for finetune, we now just need to create a trainer and start training with a smaller learning rate for fewer epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nBmF6tQITSRl"
   },
   "outputs": [],
   "source": [
    "# Setup the new trainer object\n",
    "# Let us modify some trainer configs for this demo\n",
    "# Checks if we have GPU available and uses it\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "trainer_config = OmegaConf.create(dict(\n",
    "    devices=1,\n",
    "    accelerator=accelerator,\n",
    "    max_epochs=5,\n",
    "    max_steps=-1,  # computed at runtime if not set\n",
    "    num_nodes=1,\n",
    "    accumulate_grad_batches=1,\n",
    "    enable_checkpointing=False,  # Provided by exp_manager\n",
    "    logger=False,  # Provided by exp_manager\n",
    "    log_every_n_steps=1,  # Interval of logging.\n",
    "    val_check_interval=1.0,  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations\n",
    "))\n",
    "print(OmegaConf.to_yaml(trainer_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRz-8-xzUHKZ"
   },
   "outputs": [],
   "source": [
    "trainer_finetune = pl.Trainer(**trainer_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EOwHTkW-UUy8"
   },
   "source": [
    "## Setting the trainer to the restored model\n",
    "Setting the trainer to the restored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FhYQQQOUPIk"
   },
   "outputs": [],
   "source": [
    "from nemo.utils.exp_manager import exp_manager\n",
    "log_dir_finetune = exp_manager(trainer_finetune, trainer_config.get(\"exp_manager\", None))\n",
    "print(log_dir_finetune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lc3fzGYVVTyi"
   },
   "source": [
    "## Fine-tune training step\n",
    "\n",
    "When fine-tuning on a truly new dataset, we will not see such a dramatic improvement in performance. However, it should still converge a little faster than if it was trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_model = nemo_asr.models.EncDecSpeakerLabelModel(cfg=finetune_config.model, trainer=trainer_finetune)\n",
    "speaker_model.maybe_init_from_pretrained_checkpoint(finetune_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the config, we keep weights of preprocessor and encoder, and attach a new decoder as mentioned in above section to match num of classes of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uFIOsuFYVLzr"
   },
   "outputs": [],
   "source": [
    "## Fine-tuning for 5 epochs¶\n",
    "trainer_finetune.fit(speaker_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: Add more data augmentation and dropout while finetuning on your data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DNidtl4VplU"
   },
   "source": [
    "# Saving .nemo file\n",
    "Now we can save the whole config and model parameters in a single .nemo and we can anytime restore from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "am5wej6-VdZW"
   },
   "outputs": [],
   "source": [
    "restored_model.save_to(os.path.join(log_dir_finetune, '..',\"titanet-large-finetune.nemo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnBhFJefV-Pf"
   },
   "outputs": [],
   "source": [
    "!ls {log_dir_finetune}/.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kVx1hNP_V_iz"
   },
   "outputs": [],
   "source": [
    "# restore from a save model\n",
    "restored_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', \"titanet-large-finetune.nemo\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80tLWTN40uaB"
   },
   "source": [
    "# Speaker Verification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VciRUIRz0y6P"
   },
   "source": [
    "Training for a speaker verification model is almost the same as the speaker recognition model with a change in the loss function. Angular Loss is a better function to train for a speaker verification model as the model is trained in an end-to-end manner with loss optimizing for embeddings cluster to be far from each other for different speaker by maximizing the angle between these clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULTjBuFI19Js"
   },
   "source": [
    "To train for verification we just need to toggle `angular` flag in `config.model.decoder.params.angular = True` else set it to `False` to train with cross-entropy loss for identification purposes. \n",
    "Once we set this, the loss will be changed to angular loss and we can follow the above steps to the model.\n",
    "Note the scale and margin values to be set for the loss function are present at `config.model.loss.scale` and `config.model.loss.margin`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcKiNEY032-t"
   },
   "source": [
    "## Extract Speaker Embeddings\n",
    "Once you have a trained model or use one of our pretrained nemo checkpoints to get speaker embeddings for any speaker.\n",
    "\n",
    "To demonstrate this we shall use `nemo_asr.models.EncDecSpeakerLabelModel` with say 5 audio_samples from our dev manifest set. This model is specifically for inference purposes to extract embeddings from a trained `.nemo` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXEzKMHf3r6-"
   },
   "outputs": [],
   "source": [
    "verification_model = nemo_asr.models.EncDecSpeakerLabelModel.restore_from(os.path.join(log_dir_finetune, '..', 'titanet-large-finetune.nemo'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-XiLHMQ8BIk"
   },
   "source": [
    "Now, we need to pass the necessary manifest_filepath and params to set up the data loader for extracting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk2vsDJk9PS8"
   },
   "outputs": [],
   "source": [
    "!head -5 {validation_manifest} > embeddings_manifest.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEd5poCr9yrP"
   },
   "outputs": [],
   "source": [
    "config.model.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.speaker_utils import embedding_normalize\n",
    "from tqdm import  tqdm\n",
    "try:\n",
    "    from torch.cuda.amp import autocast\n",
    "except ImportError:\n",
    "    from contextlib import contextmanager\n",
    "\n",
    "    @contextmanager\n",
    "    def autocast(enabled=None):\n",
    "        yield\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIHok6LD8g0F"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(speaker_model, manifest_file, batch_size=1, embedding_dir='./', device='cuda'):\n",
    "    test_config = OmegaConf.create(\n",
    "        dict(\n",
    "            manifest_filepath=manifest_file,\n",
    "            sample_rate=16000,\n",
    "            labels=None,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            time_length=20,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    speaker_model.setup_test_data(test_config)\n",
    "    speaker_model = speaker_model.to(device)\n",
    "    speaker_model.eval()\n",
    "\n",
    "    all_embs=[]\n",
    "    out_embeddings = {}\n",
    "           \n",
    "    for test_batch in tqdm(speaker_model.test_dataloader()):\n",
    "        test_batch = [x.to(device) for x in test_batch]\n",
    "        audio_signal, audio_signal_len, labels, slices = test_batch\n",
    "        with autocast():\n",
    "            _, embs = speaker_model.forward(input_signal=audio_signal, input_signal_length=audio_signal_len)\n",
    "            emb_shape = embs.shape[-1]\n",
    "            embs = embs.view(-1, emb_shape)\n",
    "            all_embs.extend(embs.cpu().detach().numpy())\n",
    "        del test_batch\n",
    "\n",
    "    all_embs = np.asarray(all_embs)\n",
    "    all_embs = embedding_normalize(all_embs)\n",
    "    with open(manifest_file, 'r') as manifest:\n",
    "        for i, line in enumerate(manifest.readlines()):\n",
    "            line = line.strip()\n",
    "            dic = json.loads(line)\n",
    "            uniq_name = '@'.join(dic['audio_filepath'].split('/')[-3:])\n",
    "            out_embeddings[uniq_name] = all_embs[i]\n",
    "\n",
    "    embedding_dir = os.path.join(embedding_dir, 'embeddings')\n",
    "    if not os.path.exists(embedding_dir):\n",
    "        os.makedirs(embedding_dir, exist_ok=True)\n",
    "\n",
    "    prefix = manifest_file.split('/')[-1].rsplit('.', 1)[-2]\n",
    "\n",
    "    name = os.path.join(embedding_dir, prefix)\n",
    "    embeddings_file = name + '_embeddings.pkl'\n",
    "    pkl.dump(out_embeddings, open(embeddings_file, 'wb'))\n",
    "    print(\"Saved embedding files to {}\".format(embedding_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2FRecqD-ln5"
   },
   "outputs": [],
   "source": [
    "manifest_filepath = os.path.join(NEMO_ROOT,'embeddings_manifest.json')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "get_embeddings(verification_model, manifest_filepath, batch_size=64,embedding_dir='./', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfjXPsjzDOgr"
   },
   "source": [
    "Embeddings are stored in dict structure with key-value pair, key being uniq_name generated based on audio_filepath of the sample present in manifest_file in `embedding_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmTeSR6jD28k"
   },
   "outputs": [],
   "source": [
    "ls ./embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.models.label_models import EncDecSpeakerLabelModel\n",
    "embeddings_model = EncDecSpeakerLabelModel.from_pretrained(\"titanet_large\")\n",
    "#embs = embeddings_model.get_embedding(\"segments_dir/speaker_0_0.0_4200.0.wav\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the manifests\n",
    "with open('train_manifest.json', 'r') as f:\n",
    "    train_manifest = [json.loads(l) for l in f.readlines()] \n",
    "with open('dev_manifest.json', 'r') as f:\n",
    "    val_manifest = [json.loads(l) for l in f.readlines()] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for item in train_manifest:\n",
    "    # # Load audio file\n",
    "    embeddings = embeddings_model.get_embedding(item['audio_filepath'])\n",
    "\n",
    "    # # Extract MFCC features\n",
    "    # mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "    # # Use the mean and standard deviation of the MFCCs as features\n",
    "    # mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    # mfccs_std = np.std(mfccs.T, axis=0)\n",
    "    features.append(embeddings.squeeze().cpu().numpy())\n",
    "    labels.append(item['label'])\n",
    "\n",
    "# Convert labels to integers\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 28.50, NNZs: 192, Bias: 0.635026, T: 348, Avg. loss: 0.194852\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 28.65, NNZs: 192, Bias: -0.026748, T: 696, Avg. loss: 0.026112\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 28.60, NNZs: 192, Bias: 0.131816, T: 1044, Avg. loss: 0.025887\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.44, NNZs: 192, Bias: 0.541714, T: 1392, Avg. loss: 0.024670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.30, NNZs: 192, Bias: 0.575395, T: 1740, Avg. loss: 0.024681\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 28.20, NNZs: 192, Bias: 0.034714, T: 2088, Avg. loss: 0.024686\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 28.12, NNZs: 192, Bias: 0.147501, T: 2436, Avg. loss: 0.023813\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 28.07, NNZs: 192, Bias: 0.881832, T: 2784, Avg. loss: 0.024828\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.02, NNZs: 192, Bias: 0.876313, T: 3132, Avg. loss: 0.023994\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 9 epochs took 0.00 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(loss=&#x27;log_loss&#x27;, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDClassifier(loss='log_loss', verbose=True)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train a simple MLP classifier\n",
    "# model = MLPClassifier(hidden_layer_sizes=(100, 100, ), max_iter=1000, verbose=True)\n",
    "model = SGDClassifier(loss=\"log_loss\", verbose=True)\n",
    "model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare validation data and evaluate the model\n",
    "features_val = []\n",
    "labels_val = []\n",
    "\n",
    "for item in val_manifest:\n",
    "    # Load audio file\n",
    "    embeddings = embeddings_model.get_embedding(item['audio_filepath'])\n",
    "    features_val.append(embeddings.squeeze().cpu().numpy())\n",
    "    labels_val.append(item['label'])\n",
    "\n",
    "labels_val = le.transform(labels_val)\n",
    "\n",
    "# Predict labels for validation set and calculate accuracy\n",
    "labels_pred = model.predict(features_val)\n",
    "print(\"Accuracy:\", accuracy_score(labels_val, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Assuming 'X_test' is your test data and 'y_test' are the true labels\n",
    "# and 'model' is your trained model\n",
    "\n",
    "labels_pred = model.predict(features_val)\n",
    "X_test = pd.DataFrame.from_records(val_manifest)\n",
    "y_pred = labels_pred\n",
    "y_test = labels_val\n",
    "\n",
    "incorrect_samples = X_test[y_pred != y_test]\n",
    "incorrect_true_labels = y_test[y_pred != y_test]\n",
    "incorrect_pred_labels = y_pred[y_pred != y_test]\n",
    "\n",
    "# To play some of these incorrect samples:\n",
    "for i in range(min(10, len(incorrect_samples))):  # play 10 or less\n",
    "    print(f'True Label: {incorrect_true_labels[i]}')\n",
    "    print(f'Predicted Label: {incorrect_pred_labels[i]}')\n",
    "    print(f'Audio: {incorrect_samples.iloc[i].audio_filepath}')\n",
    "    display(Audio(incorrect_samples.iloc[i].audio_filepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"grey_vs_brady.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Speaker_Recogniton_Verification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
