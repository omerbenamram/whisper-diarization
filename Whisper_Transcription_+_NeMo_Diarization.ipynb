{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahmoudAshraf97/whisper-diarization/blob/main/Whisper_Transcription_%2B_NeMo_Diarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eCmjcOc9yEtQ"
      },
      "source": [
        "# Installing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YzhncHP0ytbQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2023-05-23 10:02:36 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
            "[NeMo W 2023-05-23 10:02:37 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wget\n",
        "from omegaconf import OmegaConf\n",
        "import json\n",
        "import shutil\n",
        "from faster_whisper import WhisperModel\n",
        "import whisperx\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile\n",
        "from nemo.collections.asr.models.msdd_models import NeuralDiarizer\n",
        "from deepmultilingualpunctuation import PunctuationModel\n",
        "import re\n",
        "import logging"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jbsUt3SwyhjD"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Se6Hc7CZygxu"
      },
      "outputs": [],
      "source": [
        "punct_model_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"bg\",\n",
        "    \"pl\",\n",
        "    \"cs\",\n",
        "    \"sk\",\n",
        "    \"sl\",\n",
        "]\n",
        "wav2vec2_langs = [\n",
        "    \"en\",\n",
        "    \"fr\",\n",
        "    \"de\",\n",
        "    \"es\",\n",
        "    \"it\",\n",
        "    \"nl\",\n",
        "    \"pt\",\n",
        "    \"ja\",\n",
        "    \"zh\",\n",
        "    \"uk\",\n",
        "    \"pt\",\n",
        "    \"ar\",\n",
        "    \"ru\",\n",
        "    \"pl\",\n",
        "    \"hu\",\n",
        "    \"fi\",\n",
        "    \"fa\",\n",
        "    \"el\",\n",
        "    \"tr\",\n",
        "]\n",
        "\n",
        "\n",
        "def create_config(output_dir):\n",
        "    DOMAIN_TYPE = \"telephonic\"  # Can be meeting or telephonic based on domain type of the audio file\n",
        "    CONFIG_FILE_NAME = f\"diar_infer_{DOMAIN_TYPE}.yaml\"\n",
        "    CONFIG_URL = f\"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}\"\n",
        "    MODEL_CONFIG = os.path.join(output_dir, CONFIG_FILE_NAME)\n",
        "    if not os.path.exists(MODEL_CONFIG):\n",
        "        MODEL_CONFIG = wget.download(CONFIG_URL, output_dir)\n",
        "\n",
        "    config = OmegaConf.load(MODEL_CONFIG)\n",
        "\n",
        "    data_dir = os.path.join(output_dir, \"data\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    config.batch_size = 16\n",
        "    meta = {\n",
        "        \"audio_filepath\": os.path.join(output_dir, \"mono_file.wav\"),\n",
        "        \"offset\": 0,\n",
        "        \"duration\": None,\n",
        "        \"label\": \"infer\",\n",
        "        \"text\": \"-\",\n",
        "        \"rttm_filepath\": None,\n",
        "        \"uem_filepath\": None,\n",
        "    }\n",
        "    with open(os.path.join(data_dir, \"input_manifest.json\"), \"w\") as fp:\n",
        "        json.dump(meta, fp)\n",
        "        fp.write(\"\\n\")\n",
        "\n",
        "    pretrained_vad = \"vad_multilingual_marblenet\"\n",
        "    pretrained_speaker_model = \"titanet_large\"\n",
        "\n",
        "    config.num_workers = 1  # Workaround for multiprocessing hanging with ipython issue\n",
        "\n",
        "    config.diarizer.manifest_filepath = os.path.join(data_dir, \"input_manifest.json\")\n",
        "    config.diarizer.out_dir = (\n",
        "        output_dir  # Directory to store intermediate files and prediction outputs\n",
        "    )\n",
        "\n",
        "    config.diarizer.speaker_embeddings.model_path = pretrained_speaker_model\n",
        "    config.diarizer.oracle_vad = (\n",
        "        False  # compute VAD provided with model_path to vad config\n",
        "    )\n",
        "    config.diarizer.clustering.parameters.oracle_num_speakers = True\n",
        "    config.diarizer.clustering.parameters.num_speakers = 2\n",
        "\n",
        "    # Here, we use our in-house pretrained NeMo VAD model\n",
        "    config.diarizer.vad.model_path = pretrained_vad\n",
        "    config.diarizer.vad.parameters.onset = 0.8\n",
        "    config.diarizer.vad.parameters.offset = 0.6\n",
        "    config.diarizer.vad.parameters.pad_offset = -0.05\n",
        "    config.diarizer.msdd_model.model_path = (\n",
        "        \"diar_msdd_telephonic\"  # Telephonic speaker diarization model\n",
        "    )\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_word_ts_anchor(s, e, option=\"start\"):\n",
        "    if option == \"end\":\n",
        "        return e\n",
        "    elif option == \"mid\":\n",
        "        return (s + e) / 2\n",
        "    return s\n",
        "\n",
        "\n",
        "def get_words_speaker_mapping(wrd_ts, spk_ts, word_anchor_option=\"start\"):\n",
        "    s, e, sp = spk_ts[0]\n",
        "    wrd_pos, turn_idx = 0, 0\n",
        "    wrd_spk_mapping = []\n",
        "    for wrd_dict in wrd_ts:\n",
        "        ws, we, wrd = (\n",
        "            int(wrd_dict[\"start\"] * 1000),\n",
        "            int(wrd_dict[\"end\"] * 1000),\n",
        "            wrd_dict[\"text\"],\n",
        "        )\n",
        "        wrd_pos = get_word_ts_anchor(ws, we, word_anchor_option)\n",
        "        while wrd_pos > float(e):\n",
        "            turn_idx += 1\n",
        "            turn_idx = min(turn_idx, len(spk_ts) - 1)\n",
        "            s, e, sp = spk_ts[turn_idx]\n",
        "            if turn_idx == len(spk_ts) - 1:\n",
        "                e = get_word_ts_anchor(ws, we, option=\"end\")\n",
        "        wrd_spk_mapping.append(\n",
        "            {\"word\": wrd, \"start_time\": ws, \"end_time\": we, \"speaker\": sp}\n",
        "        )\n",
        "    return wrd_spk_mapping\n",
        "\n",
        "\n",
        "sentence_ending_punctuations = \".?!\"\n",
        "\n",
        "\n",
        "def get_first_word_idx_of_sentence(word_idx, word_list, speaker_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    left_idx = word_idx\n",
        "    while (\n",
        "        left_idx > 0\n",
        "        and word_idx - left_idx < max_words\n",
        "        and speaker_list[left_idx - 1] == speaker_list[left_idx]\n",
        "        and not is_word_sentence_end(left_idx - 1)\n",
        "    ):\n",
        "        left_idx -= 1\n",
        "\n",
        "    return left_idx if left_idx == 0 or is_word_sentence_end(left_idx - 1) else -1\n",
        "\n",
        "\n",
        "def get_last_word_idx_of_sentence(word_idx, word_list, max_words):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0 and word_list[x][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    right_idx = word_idx\n",
        "    while (\n",
        "        right_idx < len(word_list)\n",
        "        and right_idx - word_idx < max_words\n",
        "        and not is_word_sentence_end(right_idx)\n",
        "    ):\n",
        "        right_idx += 1\n",
        "\n",
        "    return (\n",
        "        right_idx\n",
        "        if right_idx == len(word_list) - 1 or is_word_sentence_end(right_idx)\n",
        "        else -1\n",
        "    )\n",
        "\n",
        "\n",
        "def get_realigned_ws_mapping_with_punctuation(\n",
        "    word_speaker_mapping, max_words_in_sentence=50\n",
        "):\n",
        "    is_word_sentence_end = (\n",
        "        lambda x: x >= 0\n",
        "        and word_speaker_mapping[x][\"word\"][-1] in sentence_ending_punctuations\n",
        "    )\n",
        "    wsp_len = len(word_speaker_mapping)\n",
        "\n",
        "    words_list, speaker_list = [], []\n",
        "    for k, line_dict in enumerate(word_speaker_mapping):\n",
        "        word, speaker = line_dict[\"word\"], line_dict[\"speaker\"]\n",
        "        words_list.append(word)\n",
        "        speaker_list.append(speaker)\n",
        "\n",
        "    k = 0\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k]\n",
        "        if (\n",
        "            k < wsp_len - 1\n",
        "            and speaker_list[k] != speaker_list[k + 1]\n",
        "            and not is_word_sentence_end(k)\n",
        "        ):\n",
        "            left_idx = get_first_word_idx_of_sentence(\n",
        "                k, words_list, speaker_list, max_words_in_sentence\n",
        "            )\n",
        "            right_idx = (\n",
        "                get_last_word_idx_of_sentence(\n",
        "                    k, words_list, max_words_in_sentence - k + left_idx - 1\n",
        "                )\n",
        "                if left_idx > -1\n",
        "                else -1\n",
        "            )\n",
        "            if min(left_idx, right_idx) == -1:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            spk_labels = speaker_list[left_idx : right_idx + 1]\n",
        "            mod_speaker = max(set(spk_labels), key=spk_labels.count)\n",
        "            if spk_labels.count(mod_speaker) < len(spk_labels) // 2:\n",
        "                k += 1\n",
        "                continue\n",
        "\n",
        "            speaker_list[left_idx : right_idx + 1] = [mod_speaker] * (\n",
        "                right_idx - left_idx + 1\n",
        "            )\n",
        "            k = right_idx\n",
        "\n",
        "        k += 1\n",
        "\n",
        "    k, realigned_list = 0, []\n",
        "    while k < len(word_speaker_mapping):\n",
        "        line_dict = word_speaker_mapping[k].copy()\n",
        "        line_dict[\"speaker\"] = speaker_list[k]\n",
        "        realigned_list.append(line_dict)\n",
        "        k += 1\n",
        "\n",
        "    return realigned_list\n",
        "\n",
        "\n",
        "def get_sentences_speaker_mapping(word_speaker_mapping, spk_ts):\n",
        "    s, e, spk = spk_ts[0]\n",
        "    prev_spk = spk\n",
        "\n",
        "    snts = []\n",
        "    snt = {\"speaker\": f\"Speaker {spk}\", \"start_time\": s, \"end_time\": e, \"text\": \"\"}\n",
        "\n",
        "    for wrd_dict in word_speaker_mapping:\n",
        "        wrd, spk = wrd_dict[\"word\"], wrd_dict[\"speaker\"]\n",
        "        s, e = wrd_dict[\"start_time\"], wrd_dict[\"end_time\"]\n",
        "        if spk != prev_spk:\n",
        "            snts.append(snt)\n",
        "            snt = {\n",
        "                \"speaker\": f\"Speaker {spk}\",\n",
        "                \"start_time\": s,\n",
        "                \"end_time\": e,\n",
        "                \"text\": \"\",\n",
        "            }\n",
        "        else:\n",
        "            snt[\"end_time\"] = e\n",
        "        snt[\"text\"] += wrd + \" \"\n",
        "        prev_spk = spk\n",
        "\n",
        "    snts.append(snt)\n",
        "    return snts\n",
        "\n",
        "\n",
        "def get_speaker_aware_transcript(sentences_speaker_mapping, f):\n",
        "    for sentence_dict in sentences_speaker_mapping:\n",
        "        sp = sentence_dict[\"speaker\"]\n",
        "        text = sentence_dict[\"text\"]\n",
        "        f.write(f\"\\n\\n{sp}: {text}\")\n",
        "\n",
        "\n",
        "def format_timestamp(\n",
        "    milliseconds: float, always_include_hours: bool = False, decimal_marker: str = \".\"\n",
        "):\n",
        "    assert milliseconds >= 0, \"non-negative timestamp expected\"\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n",
        "    return (\n",
        "        f\"{hours_marker}{minutes:02d}:{seconds:02d}{decimal_marker}{milliseconds:03d}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def write_srt(transcript, file):\n",
        "    \"\"\"\n",
        "    Write a transcript to a file in SRT format.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, segment in enumerate(transcript, start=1):\n",
        "        # write srt lines\n",
        "        print(\n",
        "            f\"{i}\\n\"\n",
        "            f\"{format_timestamp(segment['start_time'], always_include_hours=True, decimal_marker=',')} --> \"\n",
        "            f\"{format_timestamp(segment['end_time'], always_include_hours=True, decimal_marker=',')}\\n\"\n",
        "            f\"{segment['speaker']}: {segment['text'].strip().replace('-->', '->')}\\n\",\n",
        "            file=file,\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "\n",
        "def cleanup(path: str):\n",
        "    \"\"\"path could either be relative or absolute.\"\"\"\n",
        "    # check if file or directory exists\n",
        "    if os.path.isfile(path) or os.path.islink(path):\n",
        "        # remove file\n",
        "        os.remove(path)\n",
        "    elif os.path.isdir(path):\n",
        "        # remove directory and all its content\n",
        "        shutil.rmtree(path)\n",
        "    else:\n",
        "        raise ValueError(\"Path {} is not a file or dir.\".format(path))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B7qWQb--1Xcw"
      },
      "source": [
        "# Options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ONlFrSnD0FOp"
      },
      "outputs": [],
      "source": [
        "# Name of the audio file\n",
        "audio_path = 'HI/HelloInternet-11.mp3'\n",
        "\n",
        "# Whether to enable music removal from speech, helps increase diarization quality but uses alot of ram\n",
        "enable_stemming = True\n",
        "\n",
        "# (choose from 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large')\n",
        "whisper_model_name = 'medium.en'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h-cY1ZEy2KVI"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZS4xXmE2NGP"
      },
      "source": [
        "## Separating music from speech using Demucs\n",
        "\n",
        "---\n",
        "\n",
        "By isolating the vocals from the rest of the audio, it becomes easier to identify and track individual speakers based on the spectral and temporal characteristics of their speech signals. Source separation is just one of many techniques that can be used as a preprocessing step to help improve the accuracy and reliability of the overall diarization process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKcgQUrAzsJZ",
        "outputId": "dc2a1d96-20da-4749-9d64-21edacfba1b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected model is a bag of 1 models. You will see that many progress bars per track.\n",
            "Separated tracks will be stored in /home/omer/Workspace/whisper-diarization/temp_outputs/htdemucs\n",
            "Separating track HI/HelloInternet-11.mp3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████████████████████████████████| 7353.45/7353.45 [03:19<00:00, 36.80seconds/s]\n",
            "Killed\n",
            "WARNING:root:Source splitting failed, using original audio file.\n"
          ]
        }
      ],
      "source": [
        "if enable_stemming:\n",
        "    # Isolate vocals from the rest of the audio\n",
        "\n",
        "    return_code = os.system(\n",
        "        f'python3 -m demucs.separate -n htdemucs --two-stems=vocals \"{audio_path}\" -o \"temp_outputs\"'\n",
        "    )\n",
        "\n",
        "    if return_code != 0:\n",
        "        logging.warning(\n",
        "            \"Source splitting failed, using original audio file.\"\n",
        "        )\n",
        "        vocal_target = audio_path\n",
        "    else:\n",
        "        vocal_target = os.path.join(\n",
        "            \"temp_outputs\", \"htdemucs\", os.path.splitext(audio_path)[0], \"vocals.wav\"\n",
        "        )\n",
        "else:\n",
        "    vocal_target = audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocal_target = audio_path"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYg9VWb22Tz8"
      },
      "source": [
        "## Transcriping audio using Whisper and realligning timestamps using Wav2Vec2\n",
        "---\n",
        "This code uses two different open-source models to transcribe speech and perform forced alignment on the resulting transcription.\n",
        "\n",
        "The first model is called OpenAI Whisper, which is a speech recognition model that can transcribe speech with high accuracy. The code loads the whisper model and uses it to transcribe the vocal_target file.\n",
        "\n",
        "The output of the transcription process is a set of text segments with corresponding timestamps indicating when each segment was spoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5-VKFn530oTl"
      },
      "outputs": [],
      "source": [
        "# Run on GPU with FP16\n",
        "whisper_model = WhisperModel(whisper_model_name, device=\"cuda\", compute_type=\"float16\")\n",
        "\n",
        "# or run on GPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
        "# or run on CPU with INT8\n",
        "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
        "\n",
        "segments, info = whisper_model.transcribe(\n",
        "    vocal_target, beam_size=1, word_timestamps=True\n",
        ")\n",
        "whisper_results = []\n",
        "for segment in segments:\n",
        "    whisper_results.append(segment._asdict())\n",
        "# clear gpu vram\n",
        "del whisper_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'seek': 2204,\n",
              " 'start': 0.0,\n",
              " 'end': 2.94,\n",
              " 'text': ' I want to keep turning away because the microphone.',\n",
              " 'tokens': [50363,\n",
              "  314,\n",
              "  765,\n",
              "  284,\n",
              "  1394,\n",
              "  6225,\n",
              "  1497,\n",
              "  780,\n",
              "  262,\n",
              "  21822,\n",
              "  13,\n",
              "  50545],\n",
              " 'temperature': 0.0,\n",
              " 'avg_logprob': -0.38712222562279813,\n",
              " 'compression_ratio': 1.46524064171123,\n",
              " 'no_speech_prob': 0.0946044921875,\n",
              " 'words': [Word(start=0.0, end=0.2, word=' I', probability=0.74169921875),\n",
              "  Word(start=0.2, end=0.28, word=' want', probability=0.27734375),\n",
              "  Word(start=0.28, end=0.42, word=' to', probability=0.99169921875),\n",
              "  Word(start=0.42, end=0.64, word=' keep', probability=0.99755859375),\n",
              "  Word(start=0.64, end=1.16, word=' turning', probability=0.99853515625),\n",
              "  Word(start=1.16, end=1.76, word=' away', probability=0.998046875),\n",
              "  Word(start=1.76, end=2.28, word=' because', probability=0.52099609375),\n",
              "  Word(start=2.28, end=2.46, word=' the', probability=0.66357421875),\n",
              "  Word(start=2.46, end=2.94, word=' microphone.', probability=0.99853515625)]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "whisper_results[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aligning the transcription with the original audio using Wav2Vec2\n",
        "---\n",
        "The second model used is called wav2vec2, which is a large-scale neural network that is designed to learn representations of speech that are useful for a variety of speech processing tasks, including speech recognition and alignment.\n",
        "\n",
        "The code loads the wav2vec2 alignment model and uses it to align the transcription segments with the original audio signal contained in the vocal_target file. This process involves finding the exact timestamps in the audio signal where each segment was spoken and aligning the text accordingly.\n",
        "\n",
        "By combining the outputs of the two models, the code produces a fully aligned transcription of the speech contained in the vocal_target file. This aligned transcription can be useful for a variety of speech processing tasks, such as speaker diarization, sentiment analysis, and language identification.\n",
        "\n",
        "If there's no Wav2Vec2 model available for your language, word timestamps generated by whisper will be used instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to align segment (\" Yes.\"): backtrack failed, resorting to original...\n",
            "Failed to align segment (\" I don't know.\"): backtrack failed, resorting to original...\n",
            "Failed to align segment (\" Bought a couple of those.\"): backtrack failed, resorting to original...\n",
            "Failed to align segment (\" I don't know.\"): backtrack failed, resorting to original...\n",
            "Failed to align segment: duration smaller than 0.02s time precision\n",
            "Failed to align segment (\" Yeah.\"): backtrack failed, resorting to original...\n"
          ]
        }
      ],
      "source": [
        "if info.language in wav2vec2_langs:\n",
        "    device = \"cuda\"\n",
        "    alignment_model, metadata = whisperx.load_align_model(\n",
        "        language_code=info.language, device=device\n",
        "    )\n",
        "    result_aligned = whisperx.align(\n",
        "        whisper_results, alignment_model, metadata, vocal_target, device\n",
        "    )\n",
        "    word_timestamps = result_aligned[\"word_segments\"]\n",
        "    # clear gpu vram\n",
        "    del alignment_model\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    word_timestamps = []\n",
        "    for segment in whisper_results:\n",
        "        for word in segment[\"words\"]:\n",
        "            word_timestamps.append({\"text\": word[2], \"start\": word[0], \"end\": word[1]})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7EEaJPsQ21Rx"
      },
      "source": [
        "## Convert audio to mono for NeMo combatibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rVPuY1VS0xN7"
      },
      "outputs": [],
      "source": [
        "signal, sample_rate = librosa.load(vocal_target, sr=None)\n",
        "ROOT = os.getcwd()\n",
        "temp_path = os.path.join(ROOT, \"temp_outputs\")\n",
        "os.makedirs(temp_path, exist_ok=True)\n",
        "soundfile.write(os.path.join(temp_path, \"mono_file.wav\"), signal, sample_rate, \"PCM_24\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gkViCf2-CV"
      },
      "source": [
        "## Speaker Diarization using NeMo MSDD Model\n",
        "---\n",
        "This code uses a model called Nvidia NeMo MSDD (Multi-scale Diarization Decoder) to perform speaker diarization on an audio signal. Speaker diarization is the process of separating an audio signal into different segments based on who is speaking at any given time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C7jIpBCH02RL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:08:05 msdd_models:1092] Loading pretrained diar_msdd_telephonic model from NGC\n",
            "[NeMo I 2023-05-23 10:08:05 cloud:58] Found existing object /home/omer/.cache/torch/NeMo/NeMo_1.17.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2023-05-23 10:08:05 cloud:64] Re-using file from: /home/omer/.cache/torch/NeMo/NeMo_1.17.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo\n",
            "[NeMo I 2023-05-23 10:08:05 common:913] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2023-05-23 10:08:05 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: true\n",
            "    \n",
            "[NeMo W 2023-05-23 10:08:05 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    \n",
            "[NeMo W 2023-05-23 10:08:05 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    emb_dir: null\n",
            "    sample_rate: 16000\n",
            "    num_spks: 2\n",
            "    soft_label_thres: 0.5\n",
            "    labels: null\n",
            "    batch_size: 15\n",
            "    emb_batch_size: 0\n",
            "    shuffle: false\n",
            "    seq_eval_mode: false\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:08:05 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-23 10:08:05 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-23 10:08:06 save_restore_connector:247] Model EncDecDiarLabelModel was successfully restored from /home/omer/.cache/torch/NeMo/NeMo_1.17.0/diar_msdd_telephonic/3c3697a0a46f945574fa407149975a13/diar_msdd_telephonic.nemo.\n",
            "[NeMo I 2023-05-23 10:08:06 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-23 10:08:06 clustering_diarizer:127] Loading pretrained vad_multilingual_marblenet model from NGC\n",
            "[NeMo I 2023-05-23 10:08:06 cloud:58] Found existing object /home/omer/.cache/torch/NeMo/NeMo_1.17.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2023-05-23 10:08:06 cloud:64] Re-using file from: /home/omer/.cache/torch/NeMo/NeMo_1.17.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo\n",
            "[NeMo I 2023-05-23 10:08:06 common:913] Instantiating model from pre-trained checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2023-05-23 10:08:06 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /manifests/ami_train_0.63.json,/manifests/freesound_background_train.json,/manifests/freesound_laughter_train.json,/manifests/fisher_2004_background.json,/manifests/fisher_2004_speech_sampled.json,/manifests/google_train_manifest.json,/manifests/icsi_all_0.63.json,/manifests/musan_freesound_train.json,/manifests/musan_music_train.json,/manifests/musan_soundbible_train.json,/manifests/mandarin_train_sample.json,/manifests/german_train_sample.json,/manifests/spanish_train_sample.json,/manifests/french_train_sample.json,/manifests/russian_train_sample.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: true\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    tarred_shard_strategy: scatter\n",
            "    augmentor:\n",
            "      shift:\n",
            "        prob: 0.5\n",
            "        min_shift_ms: -10.0\n",
            "        max_shift_ms: 10.0\n",
            "      white_noise:\n",
            "        prob: 0.5\n",
            "        min_level: -90\n",
            "        max_level: -46\n",
            "        norm: true\n",
            "      noise:\n",
            "        prob: 0.5\n",
            "        manifest_path: /manifests/noise_0_1_musan_fs.json\n",
            "        min_snr_db: 0\n",
            "        max_snr_db: 30\n",
            "        max_gain_db: 300.0\n",
            "        norm: true\n",
            "      gain:\n",
            "        prob: 0.5\n",
            "        min_gain_dbfs: -10.0\n",
            "        max_gain_dbfs: 10.0\n",
            "        norm: true\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2023-05-23 10:08:06 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /manifests/ami_dev_0.63.json,/manifests/freesound_background_dev.json,/manifests/freesound_laughter_dev.json,/manifests/ch120_moved_0.63.json,/manifests/fisher_2005_500_speech_sampled.json,/manifests/google_dev_manifest.json,/manifests/musan_music_dev.json,/manifests/mandarin_dev.json,/manifests/german_dev.json,/manifests/spanish_dev.json,/manifests/french_dev.json,/manifests/russian_dev.json\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 256\n",
            "    shuffle: false\n",
            "    val_loss_idx: 0\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2023-05-23 10:08:06 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    labels:\n",
            "    - background\n",
            "    - speech\n",
            "    batch_size: 128\n",
            "    shuffle: false\n",
            "    test_loss_idx: 0\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:08:06 features:287] PADDING: 16\n",
            "[NeMo I 2023-05-23 10:08:07 save_restore_connector:247] Model EncDecClassificationModel was successfully restored from /home/omer/.cache/torch/NeMo/NeMo_1.17.0/vad_multilingual_marblenet/670f425c7f186060b7a7268ba6dfacb2/vad_multilingual_marblenet.nemo.\n",
            "[NeMo I 2023-05-23 10:08:07 msdd_models:864] Multiscale Weights: [1, 1, 1, 1, 1]\n",
            "[NeMo I 2023-05-23 10:08:07 msdd_models:865] Clustering Parameters: {\n",
            "        \"oracle_num_speakers\": 2,\n",
            "        \"max_num_speakers\": 8,\n",
            "        \"enhanced_count_thres\": 80,\n",
            "        \"max_rp_threshold\": 0.25,\n",
            "        \"sparse_search_volume\": 30,\n",
            "        \"maj_vote_spk_count\": false\n",
            "    }\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2023-05-23 10:08:07 clustering_diarizer:411] Deleting previous clustering diarizer outputs.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:08:07 speaker_utils:93] Number of files to diarize: 1\n",
            "[NeMo I 2023-05-23 10:08:07 clustering_diarizer:309] Split long audio file to avoid CUDA memory issue\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "splitting manifest: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:08:11 vad_utils:101] The prepared manifest file exists. Overwriting!\n",
            "[NeMo I 2023-05-23 10:08:11 classification_models:263] Perform streaming frame-level VAD\n",
            "[NeMo I 2023-05-23 10:08:11 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:08:11 collections:301] Dataset loaded with 147 items, total duration of  2.05 hours.\n",
            "[NeMo I 2023-05-23 10:08:11 collections:303] # 147 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "vad: 100%|██████████| 147/147 [00:50<00:00,  2.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:09:01 clustering_diarizer:250] Generating predictions with overlapping input segments\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                                                               "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:09:52 clustering_diarizer:262] Converting frame level prediction to speech/no-speech segment in start and end times format.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "creating speech segments: 100%|██████████| 1/1 [00:04<00:00,  4.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:09:57 clustering_diarizer:287] Subsegmentation for embedding extraction: scale0, /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/subsegments_scale0.json\n",
            "[NeMo I 2023-05-23 10:09:57 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-23 10:09:57 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:09:57 collections:301] Dataset loaded with 7151 items, total duration of  2.62 hours.\n",
            "[NeMo I 2023-05-23 10:09:57 collections:303] # 7151 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[1/5] extract embeddings: 100%|██████████| 447/447 [00:13<00:00, 32.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:10:11 clustering_diarizer:389] Saved embedding files to /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-23 10:10:11 clustering_diarizer:287] Subsegmentation for embedding extraction: scale1, /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/subsegments_scale1.json\n",
            "[NeMo I 2023-05-23 10:10:11 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-23 10:10:12 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:10:12 collections:301] Dataset loaded with 8666 items, total duration of  2.72 hours.\n",
            "[NeMo I 2023-05-23 10:10:12 collections:303] # 8666 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2/5] extract embeddings: 100%|██████████| 542/542 [00:14<00:00, 36.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:10:27 clustering_diarizer:389] Saved embedding files to /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-23 10:10:27 clustering_diarizer:287] Subsegmentation for embedding extraction: scale2, /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/subsegments_scale2.json\n",
            "[NeMo I 2023-05-23 10:10:27 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-23 10:10:27 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:10:27 collections:301] Dataset loaded with 10885 items, total duration of  2.81 hours.\n",
            "[NeMo I 2023-05-23 10:10:27 collections:303] # 10885 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[3/5] extract embeddings: 100%|██████████| 681/681 [00:18<00:00, 37.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:10:46 clustering_diarizer:389] Saved embedding files to /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-23 10:10:46 clustering_diarizer:287] Subsegmentation for embedding extraction: scale3, /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/subsegments_scale3.json\n",
            "[NeMo I 2023-05-23 10:10:46 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-23 10:10:46 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:10:46 collections:301] Dataset loaded with 14675 items, total duration of  2.91 hours.\n",
            "[NeMo I 2023-05-23 10:10:46 collections:303] # 14675 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[4/5] extract embeddings: 100%|██████████| 918/918 [00:23<00:00, 38.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:11:11 clustering_diarizer:389] Saved embedding files to /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/embeddings\n",
            "[NeMo I 2023-05-23 10:11:11 clustering_diarizer:287] Subsegmentation for embedding extraction: scale4, /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/subsegments_scale4.json\n",
            "[NeMo I 2023-05-23 10:11:11 clustering_diarizer:343] Extracting embeddings for Diarization\n",
            "[NeMo I 2023-05-23 10:11:11 collections:298] Filtered duration for loading collection is 0.000000.\n",
            "[NeMo I 2023-05-23 10:11:11 collections:301] Dataset loaded with 22492 items, total duration of  3.03 hours.\n",
            "[NeMo I 2023-05-23 10:11:11 collections:303] # 22492 files loaded accounting to # 1 labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[5/5] extract embeddings: 100%|██████████| 1406/1406 [00:36<00:00, 38.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2023-05-23 10:11:52 clustering_diarizer:389] Saved embedding files to /home/omer/Workspace/whisper-diarization/temp_outputs/speaker_outputs/embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "clustering:   0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Provided option as oracle num of speakers but num_speakers in manifest is null",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize NeMo MSDD diarization model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m msdd_model \u001b[39m=\u001b[39m NeuralDiarizer(cfg\u001b[39m=\u001b[39mcreate_config(temp_path))\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m msdd_model\u001b[39m.\u001b[39;49mdiarize()\n\u001b[1;32m      5\u001b[0m \u001b[39mdel\u001b[39;00m msdd_model\n\u001b[1;32m      6\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py:1180\u001b[0m, in \u001b[0;36mNeuralDiarizer.diarize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m   1174\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiarize\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[List[Optional[List[Tuple[DiarizationErrorRate, Dict]]]]]:\n\u001b[1;32m   1175\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[39m    Launch diarization pipeline which starts from VAD (or a oracle VAD stamp generation), initialization clustering and multiscale diarization decoder (MSDD).\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[39m    Note that the result of MSDD can include multiple speakers at the same time. Therefore, RTTM output of MSDD needs to be based on `make_rttm_with_overlap()`\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[39m    function that can generate overlapping timestamps. `self.run_overlap_aware_eval()` function performs DER evaluation.\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclustering_embedding\u001b[39m.\u001b[39;49mprepare_cluster_embs_infer()\n\u001b[1;32m   1181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmsdd_model\u001b[39m.\u001b[39mpairwise_infer \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_emb_clus_infer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclustering_embedding)\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py:699\u001b[0m, in \u001b[0;36mClusterEmbedding.prepare_cluster_embs_infer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \u001b[39mLaunch clustering diarizer to prepare embedding vectors and clustering results.\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_num_speakers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg_diar_infer\u001b[39m.\u001b[39mdiarizer\u001b[39m.\u001b[39mclustering\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mmax_num_speakers\n\u001b[0;32m--> 699\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_sess_test_dict, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_seq_test, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclus_test_label_dict, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_clustering_diarizer(\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cfg_msdd\u001b[39m.\u001b[39;49mtest_ds\u001b[39m.\u001b[39;49mmanifest_filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cfg_msdd\u001b[39m.\u001b[39;49mtest_ds\u001b[39m.\u001b[39;49memb_dir\n\u001b[1;32m    701\u001b[0m )\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/msdd_models.py:866\u001b[0m, in \u001b[0;36mClusterEmbedding.run_clustering_diarizer\u001b[0;34m(self, manifest_filepath, emb_dir)\u001b[0m\n\u001b[1;32m    864\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMultiscale Weights: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclus_diar_model\u001b[39m.\u001b[39mmultiscale_args_dict[\u001b[39m'\u001b[39m\u001b[39mmultiscale_weights\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    865\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mClustering Parameters: \u001b[39m\u001b[39m{\u001b[39;00mclustering_params_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 866\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclus_diar_model\u001b[39m.\u001b[39;49mdiarize(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfg_diar_infer\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[1;32m    868\u001b[0m \u001b[39m# If RTTM (ground-truth diarization annotation) files do not exist, scores is None.\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[39mif\u001b[39;00m scores \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/nemo/collections/asr/models/clustering_diarizer.py:456\u001b[0m, in \u001b[0;36mClusteringDiarizer.diarize\u001b[0;34m(self, paths2audio_files, batch_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m embs_and_timestamps \u001b[39m=\u001b[39m get_embs_and_timestamps(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiscale_embeddings_and_timestamps, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiscale_args_dict\n\u001b[1;32m    453\u001b[0m )\n\u001b[1;32m    455\u001b[0m \u001b[39m# Clustering\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m all_reference, all_hypothesis \u001b[39m=\u001b[39m perform_clustering(\n\u001b[1;32m    457\u001b[0m     embs_and_timestamps\u001b[39m=\u001b[39;49membs_and_timestamps,\n\u001b[1;32m    458\u001b[0m     AUDIO_RTTM_MAP\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mAUDIO_RTTM_MAP,\n\u001b[1;32m    459\u001b[0m     out_rttm_dir\u001b[39m=\u001b[39;49mout_rttm_dir,\n\u001b[1;32m    460\u001b[0m     clustering_params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cluster_params,\n\u001b[1;32m    461\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_speaker_model\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    462\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    463\u001b[0m )\n\u001b[1;32m    464\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mOutputs are saved in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m directory\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_diarizer_params\u001b[39m.\u001b[39mout_dir)))\n\u001b[1;32m    466\u001b[0m \u001b[39m# Scoring\u001b[39;00m\n",
            "File \u001b[0;32m~/Workspace/whisper-diarization/.venv/lib/python3.10/site-packages/nemo/collections/asr/parts/utils/speaker_utils.py:480\u001b[0m, in \u001b[0;36mperform_clustering\u001b[0;34m(embs_and_timestamps, AUDIO_RTTM_MAP, out_rttm_dir, clustering_params, device, verbose)\u001b[0m\n\u001b[1;32m    478\u001b[0m     num_speakers \u001b[39m=\u001b[39m audio_rttm_values\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mnum_speakers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m num_speakers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mProvided option as oracle num of speakers but num_speakers in manifest is null\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     num_speakers \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n",
            "\u001b[0;31mValueError\u001b[0m: Provided option as oracle num of speakers but num_speakers in manifest is null"
          ]
        }
      ],
      "source": [
        "# Initialize NeMo MSDD diarization model\n",
        "msdd_model = NeuralDiarizer(cfg=create_config(temp_path)).to(\"cuda\")\n",
        "msdd_model.diarize()\n",
        "\n",
        "del msdd_model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NmkZYaDAEOAg"
      },
      "source": [
        "## Mapping Spekers to Sentences According to Timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E65LUGQe02zw"
      },
      "outputs": [],
      "source": [
        "# Reading timestamps <> Speaker Labels mapping\n",
        "\n",
        "speaker_ts = []\n",
        "with open(os.path.join(temp_path, \"pred_rttms\", \"mono_file.rttm\"), \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line_list = line.split(\" \")\n",
        "        s = int(float(line_list[5]) * 1000)\n",
        "        e = s + int(float(line_list[8]) * 1000)\n",
        "        speaker_ts.append([s, e, int(line_list[11].split(\"_\")[-1])])\n",
        "\n",
        "wsm = get_words_speaker_mapping(word_timestamps, speaker_ts, \"start\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ruxc8S1EXtW"
      },
      "source": [
        "## Realligning Speech segments using Punctuation\n",
        "---\n",
        "\n",
        "This code provides a method for disambiguating speaker labels in cases where a sentence is split between two different speakers. It uses punctuation markings to determine the dominant speaker for each sentence in the transcription.\n",
        "\n",
        "```\n",
        "Speaker A: It's got to come from somewhere else. Yeah, that one's also fun because you know the lows are\n",
        "Speaker B: going to suck, right? So it's actually it hits you on both sides.\n",
        "```\n",
        "\n",
        "For example, if a sentence is split between two speakers, the code takes the mode of speaker labels for each word in the sentence, and uses that speaker label for the whole sentence. This can help to improve the accuracy of speaker diarization, especially in cases where the Whisper model may not take fine utterances like \"hmm\" and \"yeah\" into account, but the Diarization Model (Nemo) may include them, leading to inconsistent results.\n",
        "\n",
        "The code also handles cases where one speaker is giving a monologue while other speakers are making occasional comments in the background. It ignores the comments and assigns the entire monologue to the speaker who is speaking the majority of the time. This provides a robust and reliable method for realigning speech segments to their respective speakers based on punctuation in the transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgfC5hA41BXu"
      },
      "outputs": [],
      "source": [
        "if info.language in punct_model_langs:\n",
        "    # restoring punctuation in the transcript to help realign the sentences\n",
        "    punct_model = PunctuationModel(model=\"kredor/punctuate-all\")\n",
        "\n",
        "    words_list = list(map(lambda x: x[\"word\"], wsm))\n",
        "\n",
        "    labled_words = punct_model.predict(words_list)\n",
        "\n",
        "    ending_puncts = \".?!\"\n",
        "    model_puncts = \".,;:!?\"\n",
        "\n",
        "    # We don't want to punctuate U.S.A. with a period. Right?\n",
        "    is_acronym = lambda x: re.fullmatch(r\"\\b(?:[a-zA-Z]\\.){2,}\", x)\n",
        "\n",
        "    for word_dict, labeled_tuple in zip(wsm, labled_words):\n",
        "        word = word_dict[\"word\"]\n",
        "        if (\n",
        "            word\n",
        "            and labeled_tuple[1] in ending_puncts\n",
        "            and (word[-1] not in model_puncts or is_acronym(word))\n",
        "        ):\n",
        "            word += labeled_tuple[1]\n",
        "            if word.endswith(\"..\"):\n",
        "                word = word.rstrip(\".\")\n",
        "            word_dict[\"word\"] = word\n",
        "\n",
        "    \n",
        "\n",
        "    wsm = get_realigned_ws_mapping_with_punctuation(wsm)\n",
        "else:\n",
        "    print(\n",
        "        f'Punctuation restoration is not available for {whisper_results[\"language\"]} language.'\n",
        "    )\n",
        "\n",
        "ssm = get_sentences_speaker_mapping(wsm, speaker_ts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vF2QAtLOFvwZ"
      },
      "source": [
        "## Cleanup and Exporing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFTyKI6B1MI0"
      },
      "outputs": [],
      "source": [
        "with open(f\"{audio_path[:-4]}.txt\", \"w\", encoding=\"utf-8-sig\") as f:\n",
        "    get_speaker_aware_transcript(ssm, f)\n",
        "\n",
        "with open(f\"{audio_path[:-4]}.srt\", \"w\", encoding=\"utf-8-sig\") as srt:\n",
        "    write_srt(ssm, srt)\n",
        "\n",
        "cleanup(temp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "os.makedirs('segments_dir', exist_ok=True)\n",
        "\n",
        "# Read the diarization output\n",
        "with open('temp_outputs/pred_rttms/mono_file.rttm', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Load the audio file\n",
        "audio = AudioSegment.from_wav('temp_outputs/mono_file.wav')\n",
        "\n",
        "# Sample rate of the audio file\n",
        "sample_rate = sf.info('temp_outputs/mono_file.wav').samplerate\n",
        "\n",
        "# List to store the manifest data\n",
        "manifest_data = []\n",
        "\n",
        "for line in lines:\n",
        "    fields = line.split()\n",
        "    speaker = fields[7]\n",
        "    start_time = float(fields[3]) * 1000  # convert to milliseconds\n",
        "    end_time = start_time + (float(fields[4]) * 1000)  # convert to milliseconds\n",
        "\n",
        "    # Slice the audio segment\n",
        "    segment = audio[start_time:end_time]\n",
        "\n",
        "    # Save the audio segment\n",
        "    segment_path = f\"segments_dir/{speaker}_{start_time}_{end_time}.wav\"\n",
        "    segment.export(segment_path, format='wav')\n",
        "\n",
        "    duration = (end_time - start_time) / 1000  # convert back to seconds\n",
        "\n",
        "    # discard samples less than 1 second\n",
        "    if duration < 1.0:\n",
        "        continue\n",
        "    \n",
        "    # Append data to the manifest\n",
        "    manifest_data.append({\n",
        "        'audio_filepath': segment_path,\n",
        "        'duration': duration,\n",
        "        'label': \"Grey\" if speaker == \"speaker_0\" else \"Brady\",\n",
        "        'text': 'N/A'\n",
        "    })\n",
        "\n",
        "# Save the manifest file\n",
        "with open('manifest.json', 'w') as f:\n",
        "    json.dump(manifest_data, f, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Set the seed so that the split is reproducible\n",
        "random.seed(123)\n",
        "\n",
        "# Read the original manifest file\n",
        "with open('manifest.json', 'r') as f:\n",
        "    manifest = json.loads(f.read())\n",
        "\n",
        "# Shuffle the manifest\n",
        "random.shuffle(manifest)\n",
        "\n",
        "# Split into train, dev and test\n",
        "total = len(manifest)\n",
        "train_cutoff = int(total * 0.8)  # 80% for training\n",
        "dev_cutoff = int(total * 0.9)  # 10% for dev, 10% for test\n",
        "\n",
        "train_manifest = manifest[:train_cutoff]\n",
        "dev_manifest = manifest[train_cutoff:dev_cutoff]\n",
        "test_manifest = manifest[dev_cutoff:]\n",
        "\n",
        "# Write out the split manifests\n",
        "with open('train_manifest.json', 'w') as f:\n",
        "    for item in train_manifest:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "with open('dev_manifest.json', 'w') as f:\n",
        "    for item in dev_manifest:\n",
        "        f.write(json.dumps(item) + '\\n')\n",
        "\n",
        "with open('test_manifest.json', 'w') as f:\n",
        "    for item in test_manifest:\n",
        "        f.write(json.dumps(item) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOyiQNkD+ROzss634BOsrSh",
      "collapsed_sections": [
        "eCmjcOc9yEtQ",
        "jbsUt3SwyhjD"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
